https://github.com/GoogleCloudPlatform/stable-diffusion-on-gcp   最新的版本: 7e9bf448e6d7698f4d50a70083a07d1cf43269cf
这个版本我clone在0noyxgithub里面了
参考: /Users/yuanxiao/workspace/0noyxgithub/stable-diffusion-on-gcp


主要是 非 agones版本 和 agones 版本

两个版本都是跑在GKE上面的
非agones版本是不带认证的 和之前测试的类似 创建好容器之后使用LB来访问

agones版本是有认证的 
获得授权后用户完成iap验证就可以访问stable diffusion webui 
模型和插件都是共享目录挂载过去的 
agones版本有两个子版本 一个是带用户插件配置保留的 一个是不带用户插件配置保留的 




大概流程:
1. 用一台ecs来部署(安装特定版本的的gcloud和terraform)
    https://cloud.google.com/sdk/docs/install#linux
    https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli
    https://cloud.google.com/filestore/docs/mounting-fileshares?hl=zh-cn&_ga=2.186721135.-692251318.1691725083&_gac=1.58519896.1691725120.Cj0KCQjwldKmBhCCARIsAP-0rfwrt3hTG34UMpZ4dqKZeiL6ih-O0Aqv3hw3lIyqM7gWzXtFzQlTwHoaAhaEEALw_wcB#linux:-mount   将nfs挂载到ecs上
    -----
    sudo mount -o rw,intr 10.106.129.210:/vol1  /mnt
    -----
    source  ~/.zshrc_google_cloud 
    ecs的ssh可以进入控制台，复制gcloud的命令进行ssh即可(输入谷歌账号的密码即可)：
    gcloud compute ssh --zone "us-central1-c" "sd-terraform-manage" --project "happyaigc"
    gcloud config set project happyaigc (输入谷歌账号的密码即可)

1.1 本地连接调试（不用esc的方式）----参考/Users/yuanxiao/workspace/0yxgithub/Stable-Diffusion-on-GCP/yx部署过程记录.txt 第四步
    
    获取gke集群的凭证
    source  ~/.zshrc_google_cloud  即可用gcloud命令
    ------------------------------------
    GKE_CLUSTER_NAME=tf-gen-gke-c8c84f4c
    REGION=us-central1
    gcloud container clusters get-credentials ${GKE_CLUSTER_NAME} --region ${REGION}
    ------------------------------------
    f*b**1**
    ------------------------------------
    检测凭证生命周期
    ------------------------------------
    GKE_CLUSTER_NAME=tf-gen-gke-c8c84f4c
    REGION=us-central1
    gcloud container clusters describe ${GKE_CLUSTER_NAME} \
        --region ${REGION} \
        --format "value(masterAuth.clusterCaCertificate)" \
        | base64 --decode \
        | openssl x509 -noout -dates
    ------------------------------------
    kubectl get fleet  # 校验


2. 用terraform-provision-infra/README_zh.md 的Agones 版本进行部署 （其实一步步来的话这里也有 Stable-Diffusion-UI-Agones/README_cn.md）

3. 准备域名sd.agones.playdayy.cn，覆盖上面terraform-provision-infra/main.tf的配置
    oauth_client_id 和 oauth_client_secret 看 Stable-Diffusion-UI-Agones/README_cn.md   “部署 IAP（identity awared proxy身份感知代理）”的部分
    -----main.tf----
    locals {
        project_id          = "happyaigc"
        oauth_client_id     = "972449351989-7nnen6jqnrd8im4ejcbj1p64fltpsurj.apps.googleusercontent.com"
        oauth_client_secret = "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"
        sd_webui_domain     = "sd.agones.playdayy.cn"
        region              = "us-central1"
        filestore_zone      = "us-central1-f" # Filestore location must be same region or zone with gke
        cluster_location    = "us-central1-f" # GKE Cluster location
        node_machine_type   = "custom-12-49152-ext"
        accelerator_type    = "nvidia-tesla-t4" # Available accelerator_type from gcloud compute accelerator-types list --format='csv(zone,name)'
        gke_num_nodes       = 3
    }
    -----main.tf----
    部署:
    根据这个的agones部分一步步的来(非agones部分不用看): /Users/yuanxiao/workspace/0yxgithub/stable-diffusion-on-gcp-no-aliyun/terraform-provision-infra/README_zh.md
    Agones 版本部分 （必须到terraform-provision-infra目录下执行命令）
      第一步 auth2登录验证， gcloud auth application-default login，  gcloud config set project PROJECT_ID
      第二步 最后的部署命令，最好一句语句执行，就是每一个分号执行一次，执行完成执行下一个分号前的命令；
      第三部 销毁命令---不用就销毁，销毁前必须先删除自己创建的其他pod，比如ubuntu，如果ubuntu挂载了pvc，会导致销毁命名无法删除pvc；
      最好一句语句执行，就是每一个分号执行一次，执行完成执行下一个分号前的命令；
    部署完成后：如果报错failed to connect to redis!
      就执行Stable-Diffusion-UI-Agones/nginx/readme_yx.txt 进行docker替换和替换nignx的deplayment即可

4. 完成后通过https://console.cloud.google.com/security/iap?referrer=search&hl=zh-cn&project=happyaigc  添加用户
   参考本目录截图[iap添加账号1.jpg, iap添加账号2.jpg]
   更换域名和iap证书是否生效参考: /Stable-Diffusion-UI-Agones/ingress-iap/readme_yx.txt
   ------------------------------------
   -----------重要二次部署记录-----------
   ------------------------------------
   如果需要自定义sdwebui二次部署(一步步来即可): /0yxgithub/stable-diffusion-on-gcp-no-aliyun/当用terraform部署完成后的二次修改记录.txt 

5. gcloud config set project happyaigc  (输入谷歌账号的密码即可)
   kubectl get all   # 获取所有资源概览
   ---
   kubectl get pods --all-namespaces -o wide # 获取所有资源所在的节点池
   ---
   kubectl get fleet
   kubectl get fleet -o yaml # 修改这个yaml，然后apply -f即可修改sd的镜像挂载等配置
   # kubectl edit fleet # 这个命令可以现场修改fleet的配置并生效---不好用
   # kubectl edit FleetAutoscaler  # 修改fleet的缩放规则
   kubectl get fleet -o yaml > haha.yaml # 修改这个yaml，然后apply -f即可修改sd的镜像挂载等配置
   kubectl apply -f haha.yaml  # 即可更新fleet，记得先用上面的命令get最新的yaml，否则回报yaml版本冲突错误
   重要步骤---需要删除fleet相关的pod(sd-agones-fleet-*)，否则pod还是原来的镜像，不会更新的
   #####---参考terraform-provision-infra/modules/agones/cloud-build/variables.tf-----
   Stable-Diffusion-UI-Agones/agones-sidecar  yaml的第一个镜像就是这里build的
   Stable-Diffusion-UI-Agones/sd-webui/  yaml的第二个镜像是这里build的
   ####
   --------获取podyaml----
   kubectl get pod  sd-agones-fleet-klxgw-fj2lz -o yaml > haha.yaml
   --------获取gameserver的状态(多个Allocated状态代表有多少人在使用sd， 一个ready状态表示buffersize为1, Scheduled表示正在安装sd镜像部署)-----
   kubectl get gs   # 重要，不要看pod的状态，这个gs是最重要的


6. 部署完成后替换域名的方法
   参考: Stable-Diffusion-UI-Agones/ingress-iap/readme_yx.txt

7. 部署完成后修改nginx转发的方法，重新打包nginx镜像
   参考: Stable-Diffusion-UI-Agones/nginx/readme_yx.txt

8. 调试以及重要理解
   参考/Users/yuanxiao/workspace/0yxgithub/Stable-Diffusion-on-GCP/yx部署过程记录.txt 的【结束语】部分,我已经拷贝到下面第一条命令里面
   ---
   cd /Users/yuanxiao/workspace/0yxgithub/stable-diffusion-on-gcp-no-aliyun
   kubectl apply -f ubuntu_test.yaml
   ----进入ubuntu---
   kubectl  exec -it   -n default $(kubectl get pods  -n default  |grep ubuntu |awk '{print $1}' |awk NR==1)   /bin/bash
   -----例如进入ubuntu清空redis，可以强制重启sd---------
   apt update
   apt install redis-tools
   # redis-cli -h 10.198.153.83 -p 6379   # redis搜索gcp的redis自己看ip
   redis-cli -h redis.private.domain -p 6379  # tf部署的
   select 0
   keys *
   hgetall accounts.google.com:yuanxiao6786@gmail.com
   del accounts.google.com:yuanxiao6786@gmail.com # 删除key
   FLUSHDB # 清空 
   ----进入fleet的agones的其中一个pod的gameserver的容器(pod描述命令: kubectl describe pod/sd-agones-fleet-lxpsk-9w2hm -n default)----
   kubectl  exec -it  -n default $(kubectl get pods  -n default  |grep sd-agones-fleet |awk '{print $1}' |awk NR==1)  -c agones-gameserver-sidecar   /bin/sh  # 这个gameserver好像没有用，只是agones自己启动的, 没有AGONES_SDK_GRPC_PORT环境变量
   kubectl  exec -it  -n default $(kubectl get pods  -n default  |grep sd-agones-fleet |awk '{print $1}' |awk NR==1)  -c simple-game-server   /bin/sh   # 这里是agones真正的gameserver, 有AGONES_SDK_GRPC_PORT环境变量
   kubectl  exec -it  -n default $(kubectl get pods  -n default  |grep sd-agones-fleet |awk '{print $1}' |awk NR==1)  -c stable-diffusion-webui   /bin/sh   # 真正的sdwebui服务器
   --------
   AGONES_SDK_GRPC_PORT环境变量的作用参考：
   https://agones.dev/site/docs/guides/client-sdks/
   用于agones发现gameserver的grpc端口, 用于agones的sdk和agones的gameserver进行通信的, 有这个才能判断pod是否是agones的gameserver
   ---
   ---
   重要理解(Stable-Diffusion-UI-Agones/nginx/sd.lua和agones的simple-game-serverde main.go部分做的事情):
        0. agones采用fleet来创建gameserver的模板, FleetAutoscaler来自动创建gameserver，参考 https://agones.dev/site/docs/getting-started/create-fleet/
        1. sd.lua中： http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation 这个api是agones的api，也可以通过yaml的方式来创建(参考知乎https://zhuanlan.zhihu.com/p/537532777解释了buffersize的作用, 笔记搜索【云原生研究fleet和agones】),用来分配gameserver的，这个接口不会创建gameserver，只会分配现存通过FleetAutoscaler创建好的gameserver,并返回这组gameserver的pod各种信息，包括各个容器开放的端口， 如果请求的labels不一样，则会创建新的服务器，如果labels存在，则会用原来的服务器，实现一人一台sd的目的, 但这个labels并不是在agones和fleet中进行判断的, 是在redis中进行判断的, 采用agones来实现pod的自动创建和伸缩比k8s自带的更方便-------这个是最关键的-------;
        2. 就算一个pod有多个容器，多个容器其实共享一个pod的ip地址，只是端口不一样, 所以 sd.lua 脚本中，只解析host，port不一样则可以区分访问的是fleet中的agones的两个容器的哪一个真正的的gameserver即可，这里stable-diffusion-webui和simple-game-server都算agones的其中一个gameserverPod的容器， 但是开放端口不一样，agones只会检测固定gameserverPod的端口，所以可以根据固定端口检查出simple-game-server是真正的agones的入口gameserver，其他的容器只是其他的逻辑服，但是这些容器之间是平行的，没有上下级关系，只是通过不同的gameserverPod端口进行访问的区别而已。-------这个理解也是相当关键的-------;
        3. main.go中也就是simple-game-server中，判断7860是否能访问了，能访问就让simple-game-server处于ready状态，在gameserverallocation接口中，才会返回分配的服务器信息，这样访问sd也不会报错502了。------------ 这个也是相当关键的---------;
        4. sd.lua 中的 sd_port 是从 Agones Allocator 获取到的 stable-diffusion-webui 容器的端口，而 gs_port 是获取到的 simple-game-server 容器的端口;
        5. sd.lua的作用是，根据游戏服务器地址是否为 "internal"，决定是否进行 DNS 查询，以获取内部地址的实际 IP 地址。
        6. 将获取到的目标地址和端口信息存储到 Redis 中，以便下次查询， 设置 Nginx 变量 ngx.var.target 为获取的pod目标地址和sd端口，
        以便后续请求转发。
        7. pod内的多容器相互通信, 直接用[127.0.0.1:容器端口]通信即可, 所以在simple-game-server的main.go里面可以用http.Get("http://127.0.0.1:7860")来访问stable-diffusion-webui容器的7860端口 ------------ 这个也是相当关键的---------;
        8. 为什么fleet.yaml会默认产生两个类似 sd-agones-fleet-*-*的pod  （后面我找到原因了，第一次部署有问题，第二次部署就正常了,参考知乎https://zhuanlan.zhihu.com/p/537532777解释了buffersize的作用, 笔记搜索【云原生研究fleet和agones】）
        因为gameserverallocation接口用用户id作为lable去获取gameserver, 如果buffersize为1，则agons会将gameserver备用1个，所以每次都开两个
        ------buffersize很重要，可以让用户快速的使用准备好的gameserver使用sdwebui
        9.重启sd的方式
          先清理redis的用户key，然后再把用户对应的gameserver的pod删除即可
        10. kubectl get gs   # 重要，不要看pod的状态，这个gs是最重要的
        
         
    思考: 后面可以多建一个service和ingress绑定stable-diffusion-nginx, 修改sd.lua读取headers，如果没有认证，则跳转到我自己的fastapi认证服务器即可，认证服务器设置header后，再跳转回来即可，这样就可以实现自己的认证, 这样就可以不走谷歌的iap那一套认证
        
   

9. 后期清理： 一台ecs和静态ip， gke用destroy resource来清理即可, filestore的清理， artifacts的清理
   手动清理请参考：Stable-Diffusion-UI-Agones/README_yx.md 的清空资源部分，可以用chatgpt去解释每一个命令清空了什么，然后到谷歌控制台手动清理
   ---
   ---不用就销毁，销毁前必须先删除自己创建的其他pod，比如ubuntu，如果ubuntu挂载了pvc，会导致销毁命名无法删除pvc


10. 重大bug解决(agones无法安装和fleet失效的问题)----
    ----------agones无法安装的问题-------
    agones-controller,agones-extensions起不来的问题，报证书错误，文件夹路径找不到需要重装agones，并且不能和当前版本一样
    # 切换安装非原来的版本---解决(cert找不到的问题)---非必要---
    # agones-controller,agones-extensions起不来的问题，报证书错误，文件夹路径找不到需要重装agones，并且不能和当前版本一样
    # --version 的版本不要和当前版本一样，当前版本可以用helm list -A查看
    helm list -A  # 可以1.33.0和1.34.0相互切换
    helm upgrade sd-agones-release --namespace agones-system -f values.yaml --version 1.33.0 agones/agones
    ----------另外的问题(get fleet DESIRED 为1， current为0的问题)-------
    #### 执行如下命令
    kubectl get fleet
    NAME              SCHEDULING   DESIRED   CURRENT   ALLOCATED   READY   AGE
    sd-agones-fleet   Packed       1         0         0           0       3d17h
    ####
    可能原因: 参考 /0yxgithub/stable-diffusion-on-gcp-no-aliyun/手动部署最新版本(230825).txt 重要知识点
    我看到kubectl get nodes的节点的age是22h，但是agones和fleet是3天前部署的，可能是最有一个节点被删除，又重新创建的，导致fleet失效
    因为gke用的突发性实例节点,突发性实例在没有使用情况或者使用的很少的情况下下，会被回收；
    还有个可能就是现在agones 也是部署在spot gpu 节点上的 随着gpu 节点的全部回收agones 组件里面的allocator controller 之类的就没法正常工作了 导致了这个错误 如果像我们后来手动部署的集群 把agones 安装在按需节点池 default-pool 里面 因为不会中断 所以agones 组件一直是正常工作状态；
    >>>>>解决方法一(未验证):先执行helm list -A看看agones的版本确定当前版本，后面的参数--version用这个版本, 重装agones更新values的值 
    helm upgrade agones agones/agones --reuse-values --set "agones.allocator.pdb.minAvailable=0" --namespace agones-system  --version 1.34.0  --skip-crds
    >>>>>解决方法二(未验证): 再加一个standard节点池, 其他配置和spot的相同; 
    参考： https://docs.google.com/document/d/1DfUXXmeOQ1o03ocl7--umrCPlNMstb0Cr0oREK2oSJE/edit?resourcekey=0-Y2n8dYbNBJuVIlz9hApLsg  这里面有如何安装agones在standard节点池的方法；
    修改 ./Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_yx.yaml的nodeSelector为standard 节点池即可;
    (这个不知道需不需要改)修改./Stable-Diffusion-UI-Agones/agones/values.yaml的allocator->nodeSelector->cloud.google.com/gke-nodepool: default-pool 为standard 节点池即可; 别改成controller和ping了;
    ####
    当重装agones后,过一段时间fleet失效，具体现象是页面一直报many Users, kubect get fleet 一直只有一个sd-agones-fleet对象还是DESIRED状态
    可以进入nginx的pod调试扩展agones的方法（解决: there is no available GameServer to allocate 报这个错的方案）
    kubectl  exec -it   -n default $(kubectl get pods  -n default  |grep nginx |awk '{print $1}' |awk NR==1)   /bin/bash
    #
    # apt update  # 然后安装想要的东西 
    # 具体fleet和agones的关系, 参考下面第11点
    curl -X POST \
    -H "Content-Type: application/json" \
    -d '{
        "namespace": "default",
        "metadata": {
            "labels": {
                "user": "yuanxiao6785"
            }
        }
    }' \
    http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation
    #
    会报错 there is no available GameServer to allocate，就是上面的状态
    解决: there is no available GameServer to allocate 报这个错的方案
    --(需要重新安装fleet)--
    方法一(测试成功--解决上面无法allocate的问题):
        a. kubectl get fleet -o yaml  > tmp_fleet_now.yaml  # 用tmp_fleet_now.yaml的image部分代替Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_yx.yaml的image部分，其他部分如果有修改也相应替换即可。
        b. kubectl get fleetautoscaler -o yaml  > tmp_fleetautoscaler_now.yaml   # 仅仅做一个备份, 因为上面的Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_yx.yaml已经有了fleetautoscaler的配置，所以这里不需要替换
        c. kubectl delete fleet sd-agones-fleet
        d. kubectl delete fleetautoscaler fleet-autoscaler-policy 
        e. kubectl apply -f Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_yx.yaml
        
    方法二(未测试):
        a. kubectl get fleet -o yaml  > tmp_fleet_now.yaml  # 删除tmp_fleet_now.yaml的status部分即可，或者用Stable-Diffusion-UI-Agones/agones/fleet-pvc.yaml， 但需要用tmp_fleet_now.yaml替换镜像部分
        b. kubectl delete fleet sd-agones-fleet  # 后面的名字记得是上面的fleet的名字
        c. kubectl get fleetautoscaler -o yaml  > tmp_fleetautoscaler_now.yaml  # 删除tmp_fleetautoscaler_now.yaml的status部分即可， 或者用Stable-Diffusion-UI-Agones/agones/fleet_autoscale.yaml， 但需要用tmp_fleetautoscaler_now.yaml替换镜像部分
        d. kubectl delete fleetautoscaler fleet-autoscaler-policy    # 后面的名字记得是上面的fleetautoscaler的名字
        e. kubectl apply -f tmp_fleet_now.yaml
        f. kubectl apply -f tmp_fleetautoscaler_now.yaml
    

------------部署资源关键字(手动删除需要搜索的)-------------------------
        函数计算：gcp搜索function即可
        部署的定时任务，gcp搜索job
        公共目录挂载,Gcp搜索filestore —--比较贵
        Redis搜索redis
        Docker搜索artifact
        Vpc搜索vpc
        实例搜索ec2 —--比较贵
        K8s搜索gke —--比较贵
        绑定证书搜索iap
        静态ip搜索ip addresses
        函数计算的代码存储地址: gcs


11. 重要知识记录:fleet,FleetAutoscaler和agones的关系
    agones是一个分布式游戏服务器管理, gameserverallocation 这个api是agones的原生api,这个api不是由fleet提供的
    参考: https://agones.dev/site/docs/advanced/allocator-service/#using-rest;
    agones里面的gameserver可以分为多种类型，可以通过label来区分；
    那么fleet的作用是什么呢?
    fleet提供了一个创建agones的gameserver的模板并提供了一组自动伸缩来管理agones的gameserver, 相当于可以用fleet来规范agones的gameserver的创建和管理；
    我们可以通过fleet来创建多个agones的gameserver的模板，比如网关，战斗服，逻辑服等等
    下面是一个演示例子:

    a. 创建两个fleet模板和fleet自动伸缩规则FleetAutoscaler
       cd /Users/yuanxiao/workspace/0yxgithub/stable-diffusion-on-gcp-no-aliyun
       kubectl apply -f ./Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_test1.yaml
       kubectl apply -f ./Stable-Diffusion-UI-Agones/agones/fleet_and_autoscale_test2.yaml
       #
       kubectl get fleet # 进行查看
       -----
       NAME                          SCHEDULING   DESIRED   CURRENT   ALLOCATED   READY   AGE
        yx-test-simple-game-server1   Packed       1         1         0           0       56s
        yx-test-simple-game-server2   Packed       1         1         0           0       47s
       -----
       #
       kubectl get gs # 可以看到两个REDAY的gs分别属于两个fleet的模板
       #
       # 查看fleet和fleetautoscaler的yaml是否正确
       kubectl get fleet yx-test-simple-game-server1 -o yaml  
       kubectl get fleet yx-test-simple-game-server2 -o yaml  

    b. 我们进入随便一个k8s内部的pod进行测试，我这里进入的是sdwebui的nginx的pod进行测试
       kubectl  exec -it   -n default $(kubectl get pods  -n default  |grep nginx |awk '{print $1}' |awk NR==1)   /bin/bash
        # 含义参考: https://agones.dev/site/docs/getting-started/create-fleet/#4-allocate-a-game-server-from-the-fleet 入门
        # 字段参考: https://agones.dev/site/docs/reference/gameserverallocation/  每个字段的解释
        # 字段参考: https://agones.dev/site/docs/advanced/allocator-service/#using-rest
        # 字段参考: https://github.com/googleforgames/agones/blob/release-1.34.0/pkg/allocation/go/allocation.swagger.json  这个才是真正的，参考allocationAllocationRequest选项即可---重要---
        # 其中labels是给服务器打入自己的属性，比如user: yuanxiao6785 
        # gameServerSelectors选择什么服务器模板进行创建，这里就是a步骤里面的yaml的选择哪一个labels的进行分配
        执行如下脚本进行服务器分配，不是创建，是分配FleetAutoscaler创建好的服务器, 如果FleetAutoscaler没有创建好服务器，这里会如下错误()
        #######
        curl -X POST \
        -H "Content-Type: application/json" \
        -d '{
            "namespace": "default",
            "metadata": {
                "labels": {
                    "user": "yuanxiao6785"
                }
            },
            "gameServerSelectors": [
                {
                    "matchLabels":{
                        "yxgameserver": "yx-test-yxgameserver1"
                    }
                }
            ]
        }' \
        http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation
        #######
        注意: agones采用fleet来创建gameserver的模板, FleetAutoscaler来自动创建gameserver，sd.lua中： http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation 这个api是agones的api，笔记搜索【云原生研究fleet和agones】),用来分配gameserver的，这个接口不会创建gameserver，只会分配现存通过FleetAutoscaler创建好的gameserver;
        如果没有gameServerSelectors选择， 该api则会选择第一个fleet模板进行创建gameserver，我已经测试过了
        #
        执行完成后，看
        kubectl get gs # 发现yx-test-simple-game-server1多了一个gs为Allocated的状态，这就是已经分配了，然后又会自动创建一个buffersize数量的reday状态的服务器
        行如下脚本进行服务器分配，不是创建，是分配FleetAutoscaler创建好的服务器, 如果FleetAutoscaler没有创建好服务器，这里会如下错误(there is no available GameServer to allocate)--重要---
        #######
        curl -X POST \
        -H "Content-Type: application/json" \
        -d '{
            "namespace": "default",
            "metadata": {
                "labels": {
                    "user": "yuanxiao6785"
                }
            },
            "gameServerSelectors": [
                {
                    "matchLabels":{
                        "yxgameserver": "yx-test-yxgameserver2"
                    }
                }
            ]
        }' \
        http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation
        #######
        执行完成后，看
        kubectl get gs # 发现yx-test-simple-game-server2多了一个gs为Allocated的状态，这就是已经分配了，然后又会自动创建一个buffersize数量的reday状态的服务器
        ######
        最后看fleet的状态
        kubectl get fleet  # 分别被分配了一个服务器，以及一个buffersize数量的reday状态的服务器
        ----
        NAME                          SCHEDULING   DESIRED   CURRENT   ALLOCATED   READY   AGE
        yx-test-simple-game-server1   Packed       2         2         1           1       29m
        yx-test-simple-game-server2   Packed       2         2         1           1       29m
        ----
    
    c. 最后清理我们的fleet和FleetAutoscaler
       # kubectl delete gs --all  # 删除所有gs，一般不这么用
       kubectl delete fleet yx-test-simple-game-server1 yx-test-simple-game-server2
       kubectl delete fleetautoscaler yx-test-fleet-autoscaler-policy1 yx-test-fleet-autoscaler-policy2
       #测试是否删除干净
       kubectl get fleet
       kubectl get fleetautoscaler
    
12. 如何在gs内部获取gs的label
    参考 /Users/yuanxiao/workspace/0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui/user-watch.py
    获取gs的label的部分
    gs的label可以在sd.lua中的下面api动态设置，可以设置成用户名即可
    http://agones-allocator.agones-system.svc.cluster.local:443/gameserverallocation


13. 如何给不同显卡的节点池安装不同的驱动(好像用不到，因为t4和g2/l4的节点池是一样的)
    kubectl get nodes --show-labels  # 可以看各个节点的标签和值
    kubectl get node gke-tf-gen-gke-c8c84-nvidia-tesla-t4--07b29247-bdgv -o yaml  # 查看具体的某个节点的yaml
    # 可以发现t4的有标签: cloud.google.com/gke-accelerator: nvidia-tesla-t4
    # T4: https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml
    # T4: https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml
    # G2/L4: https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml
    如果想给不同的节点池，比如T4节点池和G2/L4节点池安装不同的驱动, 可以修改daemonset-preloaded-latest.yaml的matchExpressions部分
    修改上面链接的yaml文件的matchExpressions部分即可
    ###############
    matchExpressions:
    - key: cloud.google.com/gke-accelerator
    operator: Exists
    - key: cloud.google.com/gke-gpu-driver-version
    operator: DoesNotExist
    ###############
    修改为下面即可，记得修改yaml为不同名称，这样就可以给不同的节点池安装不一样的驱动
    ###############
    - matchExpressions:
    - key: cloud.google.com/gke-accelerator
        operator: In
        values:
        - nvidia-tesla-t4
    - key: cloud.google.com/gke-gpu-driver-version
        operator: DoesNotExist
    ###############

    最后在fleet.yaml中的nodeSelector中加入多个节点池即可；


14. 多端口的dockerfile对应k8s的yaml
    dockefile配置
    ##########################
    EXPOSE 30001 30002 30003   
    或者
    EXPOSE 30001
    EXPOSE 30002
    EXPOSE 30003
    ##########################

    k8s的pod配置
    ##########################
    spec:
    nodeName: cn-node1
    containers:
    - name: nginx
    - images: nginx
      ports:
      - containerPort: 30001
        protocol: TCP
      - containerPort: 30002
        protocol: UDP
      - containerPort: 30003
        protocol: TCP
    ##########################

15. 问清楚nvidia.com/gpu: "1"可以共享gpu吗，不需要限制cpu和memory吗
    参考: https://cloud.google.com/kubernetes-engine/docs/how-to/timesharing-gpus?hl=zh-cn
    https://cloud.google.com/kubernetes-engine/docs/how-to/timesharing-gpus#enable-node-pool
    -----
    创建集群的时候首先就要设置共享gpu，terraform里面有配置
    需要在/0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx/sd-agones-fleet-std.yaml
    里面限制cpu和内存，即可共享gpu
    你可以根据节点资源的情况均分一下设置cpu和mem的限制
    资源request 和 limit 写上 (共享gpu才会生效)
        nvidia.com/gpu: "1"


16. 部署多种GPU的节点池，T4, L4等
     还有个问题想请教下，我如果想再做一个L4机型的节点池，在T4没有的情况下，用L4节点池。 可以用nodeselect来实现不
     类似这样：
     kind: fleet
     nodeSelector:
            cloud.google.com/gke-nodepool: nvidia-tesla-t4-nodepool
            cloud.google.com/gke-nodepool: nvidia-L4-nodepool
     回答:
         配置两个节点池就行 提前把L4的显卡驱动装好
         node selector里面把两个GPU类型都写上 
         其实比较推荐的做法是你们选好一款gpu 然后创建两个节点池 一个标准 一个spot gke会以成本优先帮你调度节点

17. 访问报502错误, gs都是ready状态, 有gs，但没有发配服务器的情况(502页面本身也会重置redis的15分钟，所以不要频繁刷新)
    因为gpu实例是突发性实例，可能被回收了，gs是新弹出来的。但是原来的用户还在使用原来的gs，导致502，等待一段时间，gs就会被回收，然后给重新分配gs，就可以了。也可以自己清理redis的key，然后删除gs，重新分配gs即可，参考第8点，删除redis的key的方法


其他----------------------------------------------------

agones安装完成后--------------------------------------------------------------------------------------------------------
https://agones.dev/site/docs/getting-started/create-gameserver/  入门, 客户端连接那一步可以在上面的调试ubuntu里面进行
################################################################################################
安装完成后，命名空间下有以下资源：
kubectl get deploy -n agones-system
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
agones-allocator    3/3     3            3           3d
agones-controller   1/1     1            1           3d
agones-ping         2/2     2            2           3d

kubectl get po -n agones-system
NAME                                 READY   STATUS    RESTARTS   AGE
agones-allocator-554c79cffc-6dpzp    1/1     Running   0          3d
agones-allocator-554c79cffc-jlp5t    1/1     Running   0          3d
agones-allocator-554c79cffc-kbn5w    1/1     Running   0          3d
agones-controller-864dd7dbc4-2ghs8   1/1     Running   0          3d
agones-ping-77b7dc578c-9dfk7         1/1     Running   0          3d
agones-ping-77b7dc578c-hhvff         1/1     Running   0          3d

kubectl get svc -n agones-system
NAME                               TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)            AGE
agones-allocator                   LoadBalancer   172.16.157.137   47.105.53.1       443:30302/TCP      3d
agones-allocator-metrics-service   ClusterIP      172.16.238.137   <none>            8080/TCP           3d
agones-controller-service          ClusterIP      172.16.150.98    <none>            443/TCP,8080/TCP   3d
agones-ping-http-service           LoadBalancer   172.16.253.193   118.190.206.167   80:30770/TCP       3d
agones-ping-udp-service            LoadBalancer   172.16.10.63     47.104.240.179    50000:32691/UDP    3d

kubectl get gs # 获取agones游戏服务器-和kubectl get gameservers一样的效果
NAME                          STATE   ADDRESS      PORT   NODE                                                  AGE
sd-agones-fleet-klxgw-zj8f4   Ready   10.10.0.70   7961   gke-tf-gen-gke-2b6ba-nvidia-tesla-t4--acd027da-bvj0   14m
sd-agones-fleet-lxpsk-2xncq   Ready   10.10.0.60   7990   gke-tf-gen-gke-2b6ba-nvidia-tesla-t4--2fb69d9c-5p4x   29m

kubectl describe gameserver  # 看gameserver的状态
################################################################################################



kubectl get fleet -o yaml 的含义----------------------------------------------------

gpt4
################################################################################################
这段YAML文件是Kubernetes的资源配置文件，用于描述一个名为sd-agones-fleet的Fleet资源。Fleet是Agones项目中的一个资源类型，用于管理游戏服务器的集群。

在这个配置文件中，有两个image字段，它们分别对应两个不同的容器：

simple-game-server：这个容器的镜像是us-central1-docker.pkg.dev/happyaigc/sd-repository-2b6bae98/game-server:tf，它可能是一个游戏服务器的主程序。

stable-diffusion-webui：这个容器的镜像是us-central1-docker.pkg.dev/happyaigc/sd-repository-2b6bae98/sd-webui:tf，它可能是一个提供Web界面的服务。

这两个容器在同一个Pod中运行，它们可以共享网络和存储资源。这种模式通常用于将相关的服务组合在一起，例如一个Web服务和它的后端数据库。

在这个配置中，simple-game-server容器监听7654端口，stable-diffusion-webui容器监听7860端口。stable-diffusion-webui容器还挂载了一个名为stable-diffusion-storage的持久卷，用于存储模型和结果数据。

Fleet资源的spec.replicas字段设置为3，表示这个Fleet会管理3个这样的Pod。spec.scheduling字段设置为Packed，表示Agones会尽量将游戏服务器集中在尽可能少的节点上。spec.strategy字段设置为RollingUpdate，表示更新游戏服务器时会使用滚动更新策略，每次最多增加25%的Pod，最多停止25%的Pod。
################################################################################################

kubectl get pod  sd-agones-fleet-klxgw-fj2lz -o yaml 的含义----------------------------------------------------

gpt4-32k

################################################################################################
这个YAML清单定义了一个Kubernetes Pod，它是用于运行一个游戏服务器的Agones框架的一部分。Agones是由Google和Ubisoft联合开发的一个开源项目，用于在Kubernetes上运行、扩展和管理多人在线游戏。

这个Pod包含三个容器：

agones-gameserver-sidecar：这个容器是Agones SDK的一部分，它用于与Agones游戏服务器控制器进行通信和管理游戏生命周期。环境变量包含了游戏服务器的名称和Pod的命名空间。此外，还定义了一些特性开关（FEATURE_GATES）。

simple-game-server：这个容器运行实际的游戏服务器。它监听UDP端口7654，而此端口映射到主机端口7235。这个容器有一个健康检查端点/gshealthz在HTTP端口8080上。

stable-diffusion-webui：这个容器运行一个web界面，可能用于游戏的管理或监控。它监听TCP端口7860，而此端口映射到主机端口7541。这个容器需要一个NVIDIA GPU资源。

这个Pod被配置为优先调度到运行游戏服务器的节点上（根据标签agones.dev/role: gameserver）。这是通过Pod亲和性（podAffinity）来实现的。

这个Pod还具有一些特殊的配置，例如：

serviceAccount: agones-sdk：Pod使用的服务账户，可能用于与Kubernetes API的交互。
tolerations：允许Pod被调度到具有特定污点（taints）的节点上，例如具有GPU的节点。
volumes：Pod使用的存储卷，包括一个持久卷（用于存储模型和结果）和一个空的临时卷。
在status部分，你可以看到Pod的运行状态，包括每个容器的状态、IP地址、启动时间，以及Pod是否已准备好接受流量。
################################################################################################




kubectl get fleet -o yaml > fleet.yaml 
fleet.yaml 会生成两个pod类似下面的sd-agones-fleet-*-*
获取pod的yaml文件
kubectl get pod  sd-agones-fleet-klxgw-fj2lz > fleet_pod.yaml

我的问题有两个:
1. 为什么fleet.yaml会默认产生两个类似 sd-agones-fleet-*-*的pod  （后面我找到原因了: 将Stable-Diffusion-UI-Agones/agones/fleet_autoscale.yaml的bufferSize设置为0即可）
2. 为什么fleet.yaml里面只有两个容器， 生成的fleet_pod.yaml里面有三个容器

############################################
当您在Kubernetes中创建一个Fleet对象时，它代表了一组Pod的控制器，该组Pod按照指定的副本数和模板创建和管理。您提供的Fleet YAML文件描述了一个Fleet对象，但似乎有一些配置导致了您观察到的现象。让我逐个回答您的问题：

    为什么会产生两个Pod？：在提供的Fleet YAML中，replicas: 1表示Fleet期望维持一个Pod副本。然而，实际产生了两个Pod的原因可能有很多，包括：
        容错机制：Kubernetes控制器，包括Fleet控制器，通常会使用容错机制，确保维持所需数量的Pod副本。如果一个Pod终止或启动失败，控制器可能会创建一个新的Pod，以满足所需状态。这可能解释了为什么您看到了两个Pod，尽管所需状态是一个。
        控制器更新：如果Fleet规范在初始创建后已经更新，Fleet控制器可能会检测到此更新，并根据更新的规范创建新的Pod。检查一下Fleet的副本数或其他相关字段是否有更新。
        手动干预：也有可能手动干预或与系统中其他组件的交互导致了额外的Pod创建。
    要进一步调查，您可能需要查看任何事件、日志或可能触发Fleet创建多个Pod的更新。

    当您创建 Fleet 对象时，生成的 fleet.yaml 文件中只定义了两个容器：simple-game-server 和 stable-diffusion-webui。然而，在生成的 fleet_pod.yaml 文件中出现了三个容器：agones-gameserver-sidecar、simple-game-server 和 stable-diffusion-webui。

agones-gameserver-sidecar是agones塞进去的，那个game server里没有跑进程（我看也没有AGONES_SDK_GRPC_PORT环境变量，所以没有用）--------谷歌人员解答--------
--
这可能是由于 Agones 游戏服务器 SDK 引入的影响。Agones 可能会自动添加一个辅助容器 agones-gameserver-sidecar 来管理游戏服务器实例的生命周期，从而导致生成的 Pod 定义中多出了一个容器。为了准确理解这个差异，您可以查阅 Agones 和 Fleet 控制器的文档，以及相关的注解和标签，以了解它们对 Pod 创建和管理的影响。
############################################




