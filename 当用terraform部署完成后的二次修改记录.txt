1. (首次执行)在自定义ubuntu里面部署filestore的公共Python环境和sd公共环境(只用部署一次，后面filesstore里面即可用这个python环境)
    ----进入ubuntu---
    kubectl  exec -it   -n default $(kubectl get pods  -n default  |grep ubuntu |awk '{print $1}' |awk NR==1)   /bin/bash
    防止vim的时候中文乱码: 搜索笔记【securecrt乱码的原因两种】看最后ubuntu的那一段即可
    ------在filestore里面安装anconda------------
    mkdir -p /yuanxiao_root_nfs/sdwebui_public/versions/sdwebui_env
    cd /yuanxiao_root_nfs/sdwebui_public
    wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.1.0-1-Linux-x86_64.sh
    bash Miniconda3-py38_23.1.0-1-Linux-x86_64.sh -b -p -f /yuanxiao_root_nfs/sdwebui_public/versions/sdwebui_env/miniconda3
    --- 激活conda环境 ---
    /yuanxiao_root_nfs/sdwebui_public/versions/sdwebui_env/miniconda3/bin/conda init
    source ~/.bashrc
    --- 创建一个虚拟环境---
    conda create -y --name sd_python310_20230907 python=3.10
    conda activate sd_python310_20230907
    pip install aws-shell==0.2.2 # 用于同步s3的模型
    --- 测试gradio是否能提供公网访问(pod内没有service)--测试成功，可以访问 ---
    0yxgithub/userful_scripts/gradio_test/test_browse_donwload_inner_fastapi_onefile.py


2. (首次执行)同步s3的模型到filestore
   笔记搜索[awss3sync常用命令sdwebui云函数nas同步备份webui]
   同步路径: /yuanxiao_root_nfs/sdwebui_public

2.0 机型选择以及查看(阅读步骤，不需要操作),如果节点池的机型需要修改，可以参考这个步骤
    ----------------
    如果需要自定义机型
    (并不是自定义一个机型模板, gcp有自己的自定义机型的格式: custom-cpu数量-内存数量, 这样就会自动创建自定义机型，而不是有custom-xx-xx这个模板): 
    https://cloud.google.com/compute/docs/instances/creating-instance-with-custom-machine-type?hl=zh-cn  普通自定义机型说明
    https://cloud.google.com/compute/docs/gpus/create-gpu-vm-accelerator-optimized?hl=zh-cn  l4的gpu的支持
    https://cloud.google.com/compute/docs/gpus/create-gpu-vm-general-purpose?hl=zh-cn   t4的gpu支持
    ----------------l4的机器类型列表
    https://cloud.google.com/compute/docs/gpus?hl=zh-cn#l4-gpus  
    ----------------
    大概格式: --custom-extensions代表-ext  --custom-cpu=2 --custom-memory=15  对应格式  custom-2-15-ext
    注意: 如果您创建自定义 N1 虚拟机，则每个 vCPU 最多可以配备 6.5 GB 的内存。对于自定义 N2 虚拟机，此数量最多增加至每个 vCPU 8 GB 的内存
          其他限制请查看上面文档
    ----------------
    查看所有的实例
    gcloud compute instances list
    ----------------

2.1 (首次执行-可选)新增一个L4的GPU类型的节点池，T4实在太少了（如果需要删除节点池，界面上也可以删除）----删除集群之前先手动删除这个节点池--
    ----------------
    PROJECT_ID=happyaigc
    GKE_CLUSTER_NAME=tf-gen-gke-c8c84f4c
    REGION=us-central1
    ZONE=us-central1-a  # 因为只有这个zone有l4的gpu
    ----------------
    # 如果加--preemptible true 表示是抢占式的，会便宜很多，但是会被回收; 如果false就是正常的，不会被回收, 但是贵很多;
    # --spot 表示是抢占式的；与 --preemptible 不同，--spot 实例的生命周期不是临时性的，而是根据 Cloud Dataprep 任务的需要动态创建和销毁。
    # 具体区别参考: https://cloud.google.com/kubernetes-engine/docs/concepts/node-auto-provisioning?hl=zh-cn#support_for_spot_vms
    # gcp的工作人员回答是: spot是新版叫法 preemptible是旧版的叫法。 建议用spot, 这两种类型中断频率不一样，但是都会被回收; preemptible 最长运行时间是24小时 超过的话就会中断一次; spot没有这个限制,资源不够了才会中断.
    # 但是老版的节点池已经用preemptible创建了， 这个能改为spot吗？gcp回答: 不能改，只能删除重建
    gcloud container node-pools create nvidia-l4-nodepool \
    --cluster ${GKE_CLUSTER_NAME} \
    --region ${REGION} \
    --node-locations ${ZONE} \
    --spot \
    --machine-type "g2-standard-12" \
    --accelerator  "type=nvidia-l4,count=1,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=2" \
    --image-type "COS_CONTAINERD" \
    --disk-type "pd-balanced" \
    --disk-size "100" \
    --metadata disable-legacy-endpoints=true \
    --scopes "https://www.googleapis.com/auth/cloud-platform" \
    --enable-autoscaling \
    --total-min-nodes "0" \
    --total-max-nodes "20" \
    --location-policy "ANY" \
    --enable-autoupgrade \
    --num-nodes "0"

    # 不需要安装gpu驱动，不会影响T4，和T4是一样的
    # kubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml

    # 如果需要装驱动，请参考 【root/最新版本的部署方式(首先看这个).txt】 的第13点

    目前不用为这个节点池安装GPU驱动，因为和T4节点池的驱动是一样的，所以不用安装(以前已经装过上面的文件了，兼容T4和L4, 遇到不兼容的再说)
    修改/0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx/sd-agones-fleet-std.yaml 里面的nodeseletor加上L4节点池的标签即可, 即 affinity部分 参考【root/最新版本的部署方式(首先看这个).txt】第16点, 然后结合第5步进行运行修改即可
    -------
    注意: 节点池第一次创建后， gameserver第一次创建可能会处于delete状态，需要等待一段时间拉取奖项，然后再次创建即可。

2.2 (首次执行-可选)删除原来nvidia-tesla-t4-nodepool的节点池，修改为spot类型
    ----------------
    PROJECT_ID=happyaigc
    GKE_CLUSTER_NAME=tf-gen-gke-c8c84f4c
    REGION=us-central1
    ZONE=us-central1-a  # 因为只有这个zone有l4的gpu
    ----------------
    # 删除节点池
    gcloud container node-pools delete nvidia-tesla-t4-nodepool \
    --cluster ${GKE_CLUSTER_NAME} \
    --region ${REGION} \
    --quiet
    ----------------
    # 重新创建节点池 （一个gpu一台用custom-4-49152-ext, 一个GPU两台用custom-12-49152-ext）
    gcloud container node-pools create nvidia-tesla-t4-nodepool \
    --cluster ${GKE_CLUSTER_NAME} \
    --region ${REGION} \
    --spot \
    --machine-type "custom-12-49152-ext" \
    --accelerator  "type=nvidia-tesla-t4,count=1,gpu-sharing-strategy=time-sharing,max-shared-clients-per-gpu=2" \
    --image-type "COS_CONTAINERD" \
    --disk-type "pd-balanced" \
    --disk-size "100" \
    --metadata disable-legacy-endpoints=true \
    --scopes "https://www.googleapis.com/auth/cloud-platform" \
    --enable-autoscaling \
    --total-min-nodes "0" \
    --total-max-nodes "20" \
    --location-policy "ANY" \
    --enable-autoupgrade \
    --num-nodes "1"
    ----------------

    # 不需要安装gpu驱动，和原来的t4节点池一样
    


3. 经常用--(sd版本变更，必选重要步骤)重新打包sdwebui并调试(后面有sd镜像升级，可以从这一步开始)
   参考打包基础镜像: /0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx/readme.txt
      
4. (可选)新增或修改支持minio的域名(可选)
   域名在: /0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx/dockerdata/nginx2.conf 里面有三个
   sdwebui和supervisor共用一个域名用nignx的rewrite来区分
   minio-web需要根路径，所以需要单独一个域名
   minio-api也需要根路径，所以需要单独一个域名
   ---
   参考 /0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/ingress-iap/readme_yx.txt  如何新增域名

5. 经常用--(sd版本变更，必选重要步骤)修改fleet的sd的挂载路径(访问supervisor在域名后面加/psuperfaa/即可)
    必须第3步完成后才做这一步
    
    第一种方法(测试成功):
        修改/0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx/sd-agones-fleet-std.yaml 里面需要修改的,特别是镜像版本，有注释---重要---
        cd /Users/yuanxiao/workspace/0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/sd-webui-yx
        -- 先删除
        kubectl delete fleet sd-agones-fleet
        kubectl delete FleetAutoscaler fleet-autoscaler-policy 
        -- 再创建 (等 kubectl get gs 没有了就开始创建)
        kubectl apply -f sd-agones-fleet-std.yaml 即可, 会重新创建gameserver， 如果需要修改自动弹缩,打开文件里面最后的注释即可
        # kubectl scale fleet sd-agones-fleet --replicas=5  # 这个命令可以修改自动弹缩的数量

    第二种方法(比较保险):
        kubectl get fleet sd-agones-fleet -o yaml > sd-agones-fleet-tmp.yaml
        修改 sd-agones-fleet-tmp.yaml
        #########
        template:
            spec:
                ...
                containers:
                - name: stable-diffusion-webui
                    image: us-central1-docker.pkg.dev/happyaigc/sd-repository-c8c84f4c/sd-webui:xxx # 修改这里为上面的镜像
                ...
                - command:   # 这里改成自己的启动命令
                - /bin/sh
                - -c
                - start_supervisor2.sh
        ...
        volumeMounts:
        - mountPath: /yuanxiao_root_nfs  # 修改这里
            name: stable-diffusion-storage
            subPath: " "
        
        #########
        kubectl apply -f sd-agones-fleet-tmp.yaml
        rm -rf sd-agones-fleet-tmp.yaml
    
    注意： 重新部署fleet后，需要参考------重要------
          /0yxgithub/stable-diffusion-on-gcp-no-aliyun/最新版本的部署方式(首先看这个).txt
          第八点 调试以及重要理解  ----- 清理redis的数据，否则会出现用户502的问题
        
    注意:  如果报错
        kubectl get gs
        NAME                          STATE            ADDRESS   PORT   NODE   AGE
        sd-agones-fleet-6t6ps-bgr9d   Unhealthy                                0s
        sd-agones-fleet-6t6ps-rdsws   PortAllocation                           0s
        需要修改一下sd-agones-fleet-std.yaml里面weight 为 100以内, 千万不要大于100，会报上面的错误

5. (可选)进入gs进行调试，或者进入gs看日志
   # 这个gs的pod因为是多容器的，所以进入的时候要指定容器，这里指定的是stable-diffusion-webui即可
   kubectl  exec -it   -n default gs的pod的name  -c stable-diffusion-webui  /bin/bash
   例如:
   kubectl  exec -it   -n default sd-agones-fleet-p4kbn-7dzw5  -c stable-diffusion-webui  /bin/bash
   # /usr/sbin/nginx -s reload  nignx重启命令

6. (可选)后期如果需要不走iap认证走自己的认证可以参考
   0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/ingress-iap/yx-clean-ingress
   自己定义带lua的nginx和重定向到自己的认证服务即可

7. (必选)后期调试和修改带lua的nignx转发用(重要的知识点),不用打包nginx镜像，直接apply yaml即可，非常方便-----重要-----
   修改转发逻辑在这里
   /0yxgithub/stable-diffusion-on-gcp-no-aliyun/Stable-Diffusion-UI-Agones/nginx/readme_yx.txt 的第二种方法即可

8. (维护)删除gs和node的方法
   kubectl get gs # 获取gs的名字和状态
   kubectl delete gs sd-agones-fleet-6t6ps-bgr9d  # 如果2个gs没有落到一个节点，想让落到一个节点，先删除一个ready状态的gs
   kubectl delete node gke-tf-gen-gke-c8c84-nvidia-l4-nodepo-6302c466-lwf8  # 然后再删除节点即可
   
9. 公司文档地址
    https://confluence.playnexx.net/pages/viewpage.action?pageId=130711851